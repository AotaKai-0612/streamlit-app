import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from openai import OpenAI
from dotenv import load_dotenv
import os
import time
import json
import re
import concurrent.futures

# 1. ç’°å¢ƒè¨­å®š ---------------------------------------------------------
load_dotenv()
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not YOUTUBE_API_KEY or not OPENAI_API_KEY:
    st.error("âŒ APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã« YOUTUBE_API_KEY ã¨ OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
client = OpenAI(api_key=OPENAI_API_KEY)

st.set_page_config(page_title="YouTubeã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ ", layout="wide")
st.title("ğŸ¥ YouTubeã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ â†’ å‹•ç”»é¸æŠ â†’ ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— â†’ GPTã§è‡ªå‹•åˆ†æï¼ˆgpt-4o-miniï¼‰")

# 2. å®šæ•°ãƒ»ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆæ—§ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰ ----------------------------------
FEATURES = [
    {"key": "æ”»æ’ƒæ€§", "min": 0, "max": 3, "desc": "ä»–è€…ã¸ã®ç›´æ¥çš„ãªæ•µæ„ãƒ»ä¾®è¾±ãƒ»è„…è¿«ã®åº¦åˆã„ã€‚0=ãªã—, 3=é«˜"},
    {"key": "æŒ‘ç™ºæ€§", "min": 0, "max": 3, "desc": "çš®è‚‰ãƒ»ç…½ã‚Šç­‰ã§åå¿œã‚’å¼•ãå‡ºã™åº¦åˆã„ã€‚0=ãªã—, 3=é«˜"},
    {"key": "æœ‰ç”¨æ€§", "min": 0, "max": 3, "desc": "å‹•ç”»ã‚„è¦–è´è€…ã«ã¨ã£ã¦æœ‰ç›Šã‹ï¼ˆ0=ãªã—, 3=é«˜ï¼‰"},
    {"key": "æ„Ÿæƒ…æ¥µæ€§", "min": -2, "max": 2, "desc": "æ„Ÿæƒ…ã®ãƒˆãƒ¼ãƒ³ã€‚-2=å¼·ã„ãƒã‚¬ãƒ†ã‚£ãƒ–, 0=ä¸­ç«‹, +2=å¼·ã„ãƒã‚¸ãƒ†ã‚£ãƒ–"},
    {"key": "è‡ªå·±é¡•ç¤ºæ€§", "min": 0, "max": 3, "desc": "è‡ªåˆ†ã®çŸ¥è­˜ã‚„çµŒæ­´ã§å„ªä½æ€§ã‚’ç¤ºã™åº¦åˆã„"},
    {"key": "æ–‡è„ˆä¾å­˜æ€§", "min": 0, "max": 3, "desc": "å†…è¼ªãƒã‚¿ã‚„å°‚é–€ç”¨èªã®é »åº¦ã€‚0=ã‚ã‹ã‚Šã‚„ã™ã„, 3=é«˜åº¦ã«ä¾å­˜"}
]

def extract_number_from_text(s):
    if s is None:
        return None
    if isinstance(s, (int, float)):
        return s
    m = re.search(r"-?\d+(\.\d+)?", str(s))
    if m:
        try:
            if '.' in m.group(0):
                return float(m.group(0))
            else:
                return int(m.group(0))
        except:
            return None
    return None

def normalize_analysis_to_row(analysis):
    row = {}
    reasons = []
    model_overall = None
    if isinstance(analysis, dict):
        model_overall = analysis.get("ç·åˆã‚³ãƒ¡ãƒ³ãƒˆ") or analysis.get("ç·åˆè©•ä¾¡") or analysis.get("ç·åˆ") or analysis.get("ç·åˆã‚³ãƒ¡ãƒ³ãƒˆï¼ˆè¦ç´„ï¼‰")
    for f in FEATURES:
        k = f["key"]
        val = None
        score = None
        reason = None
        if isinstance(analysis, dict):
            val = analysis.get(k)
        if isinstance(val, dict):
            score = val.get("score") if "score" in val else extract_number_from_text(val.get("value") or val.get("level") or None)
            reason = val.get("reason") or val.get("explanation") or None
        elif isinstance(val, (int, float)):
            score = val
        elif isinstance(val, str):
            score = extract_number_from_text(val)
            reason = re.sub(r"-?\d+(\.\d+)?", "", val).strip(" :,-ã€‚ï¼")
            if reason == "":
                reason = None
        if score is None and isinstance(analysis, dict):
            alt_key = f"{k}_score"
            if alt_key in analysis:
                score = extract_number_from_text(analysis.get(alt_key))
            alt2 = k.lower() + "_score"
            if score is None and alt2 in analysis:
                score = extract_number_from_text(analysis.get(alt2))
        row[f"{k}_score"] = score
        if reason:
            reasons.append(f"{k}ï¼š{reason}")
    if model_overall and isinstance(model_overall, str) and model_overall.strip():
        overall = model_overall
    else:
        if reasons:
            overall = "ãƒ¢ãƒ‡ãƒ«ç†ç”±ã«åŸºã¥ãç·åˆã‚³ãƒ¡ãƒ³ãƒˆ â€” " + "ï¼›".join(reasons[:6])
        else:
            if isinstance(analysis, dict) and "raw_output" in analysis:
                overall = f"ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼ˆéJSONï¼‰: {str(analysis['raw_output'])[:300]}"
            elif isinstance(analysis, dict) and any(k not in [f"{ft['key']}_score" for ft in FEATURES] for k in analysis.keys()):
                overall = "ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›: " + ", ".join(list(analysis.keys())[:5])
            else:
                overall = "è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸç·åˆã‚³ãƒ¡ãƒ³ãƒˆï¼šè©³ç´°ãªç†ç”±ãŒãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
    row["ç·åˆã‚³ãƒ¡ãƒ³ãƒˆ"] = overall
    return row

# 3. APIé–¢é€£é–¢æ•° ---------------------------------------

def search_videos(query, max_results=6, page_token=None):
    try:
        req = youtube.search().list(
            part="snippet", q=query, type="video",
            videoEmbeddable="true", maxResults=max_results, order="relevance",
            pageToken=page_token 
        )
        res = req.execute()
    except Exception as e:
        st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return [], None
    
    results = []
    for item in res.get("items", []):
        vid = item.get("id", {}).get("videoId")
        if not vid: continue
        snip = item.get("snippet", {})
        results.append({
            "title": snip.get("title"),
            "video_id": vid,
            "thumbnail": snip.get("thumbnails", {}).get("medium", {}).get("url")
        })
    
    next_token = res.get("nextPageToken")
    return results, next_token

# ã€ä¿®æ­£ã€‘120ä»¶å–å¾—ã™ã‚‹ãƒ«ãƒ¼ãƒ—å‡¦ç†ï¼ˆ100ä»¶è¡¨ç¤ºã®äºˆå‚™ã®ãŸã‚ï¼‰
def get_comments(video_id, max_comments=120):
    comments = []
    try:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100, # APIã®1ãƒšãƒ¼ã‚¸æœ€å¤§æ•°
            textFormat="plainText",
            order="relevance"
        )
        while request and len(comments) < max_comments:
            response = request.execute()
            for item in response.get("items", []):
                try:
                    c = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
                    comments.append(c)
                    if len(comments) >= max_comments:
                        break
                except KeyError:
                    continue
            
            # ã¾ã è¶³ã‚Šãªã‘ã‚Œã°æ¬¡ãƒšãƒ¼ã‚¸ã¸
            if len(comments) < max_comments:
                request = youtube.commentThreads().list_next(request, response)
            else:
                break
    except Exception as e:
        st.warning(f"ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    return comments[:max_comments]

# ã€ä¿®æ­£ã€‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ç·©å’Œç‰ˆã«å¤‰æ›´ï¼ˆå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã¯æ—§ã‚³ãƒ¼ãƒ‰ã®ã¾ã¾ï¼‰
def analyze_comment(comment_text):
    prompt = f"""
    ã‚ãªãŸã¯YouTubeã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
    ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«ã€å³å¯†ã«ã€‘å¾“ã£ã¦ã€æŒ‡å®šã•ã‚ŒãŸYouTubeã‚³ãƒ¡ãƒ³ãƒˆã‚’6ã¤ã®ç‰¹å¾´é‡ã§åˆ†æã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    æ–‡è„ˆã‚„çš®è‚‰ï¼ˆåèªï¼‰ã‚‚è€ƒæ…®ã—ã¦è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

    # åˆ†æãƒ«ãƒ¼ãƒ«ï¼ˆè©•ä¾¡åŸºæº–ã®ç·©å’Œç‰ˆï¼‰

    ## 1. æ”»æ’ƒæ€§ (Aggressiveness)
    - **0: ãªã—**: æ•¬æ„ãŒã‚ã‚‹ã€ã¾ãŸã¯ä¸­ç«‹çš„ã€‚
    - **1: ä½**: å°é¦¬é¹¿ã«ã™ã‚‹ã€è¦‹ä¸‹ã™ã‚ˆã†ãªè¡¨ç¾ã€‚è»½ã„ä¸å¿«æ„Ÿã€‚
    - **2: ä¸­**: æ˜ç¢ºãªæ‚ªå£ã€å¼·ã„å˜²ç¬‘ã€‚ã€Œãƒã‚«ã€ã€Œã‚´ãƒŸã€ãªã©ã®ä¾®è¾±èªã€‚
    - **3: é«˜**: **éå¸¸ã«æ¿€ã—ã„æ•µæ„ã€äººæ ¼å¦å®šã€ã¾ãŸã¯åŸ·æ‹—ãªæ”»æ’ƒã€‚**

    ## 2. æŒ‘ç™ºæ€§ (Provocation)
    - **0: ãªã—**: èª å®Ÿã§ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆãªè¡¨ç¾ã€‚
    - **1: ä½**: è»½åº¦ã®çš®è‚‰ã‚„å«Œå‘³ã€‚
    - **2: ä¸­**: ä¸Šã‹ã‚‰ç›®ç·šã€ãƒ¬ãƒƒãƒ†ãƒ«è²¼ã‚Šã€‚
    - **3: é«˜**: **ç›¸æ‰‹ã‚’æ¿€æ˜‚ã•ã›ã‚‹ã“ã¨ã‚’ä¸»ç›®çš„ã¨ã—ãŸå¼·ã„ç…½ã‚Šã€æ‚ªè³ªãªå˜²ç¬‘ã€‚**

    ## 3. æœ‰ç”¨æ€§ (Usefulness)
    - **0: ãªã—**: ä¸­èº«ã®ãªã„ç›¸æ§Œã€å˜ãªã‚‹æ„Ÿæƒ…è¡¨ç¾ã€‚
    - **1: ä½**: æ ¹æ‹ ã®ãªã„å€‹äººã®æ„Ÿæƒ³ã€‚
    - **2: ä¸­**: å…·ä½“çš„ãªæŒ‡æ‘˜ã€æ”¹å–„ææ¡ˆã€ç†ç”±ã‚’ä¼´ã†æ„è¦‹ã€‚
    - **3: é«˜**: **éå¸¸ã«è«–ç†çš„ã§ã€å…·ä½“çš„ãªæ ¹æ‹ ã‚„ç‹¬è‡ªã®æ·±ã„è¦–ç‚¹ã«åŸºã¥ãã€è­°è«–ã«è²¢çŒ®ã™ã‚‹æ¥µã‚ã¦æœ‰ç›Šãªã‚³ãƒ¡ãƒ³ãƒˆã€‚**

    ## 4. æ„Ÿæƒ…æ¥µæ€§ (Sentiment Polarity)
    - **-2: å¼·ã„ãƒã‚¬ãƒ†ã‚£ãƒ–**: æ¿€æ€’ã€å¼·ã„æ†ã—ã¿ã€‚
    - **-1: ãƒã‚¬ãƒ†ã‚£ãƒ–**: æ‰¹åˆ¤ã€ä¸æº€ã€‚
    - **0: ä¸­ç«‹**: äº‹å®Ÿã®è¨˜è¿°ã€‚
    - **+1: ãƒã‚¸ãƒ†ã‚£ãƒ–**: å¥½æ„ã€æ„Ÿè¬ã€‚
    - **+2: å¼·ã„ãƒã‚¸ãƒ†ã‚£ãƒ–**: æ„Ÿå‹•ã€çµ¶è³›ã€‚

    ## 5. è‡ªå·±é¡•ç¤ºæ€§ (Self-display)
    - **0: ãªã—**: ã‚¢ãƒ”ãƒ¼ãƒ«ãªã—ã€‚
    - **1: ä½**: æ–‡è„ˆã«æ²¿ã£ãŸä½“é¨“è«‡ã€‚
    - **2: ä¸­**: çŸ¥è­˜ã²ã‘ã‚‰ã‹ã—ã€æš—é»™ã®ãƒã‚¦ãƒ³ãƒˆã€‚
    - **3: é«˜**: **éœ²éª¨ãªãƒã‚¦ãƒ³ãƒˆã€è¦‹ä¸‹ã—ã€‚**

    ## 6. æ–‡è„ˆä¾å­˜æ€§ (Context-dependency)
    - **0: ãªã—**: èª°ã§ã‚‚ã‚ã‹ã‚‹ã€‚
    - **1: ä½**: æ¨æ¸¬å¯èƒ½ã€‚
    - **2: ä¸­**: ãƒ•ã‚¡ãƒ³ç”¨èªã€ãƒŸãƒ¼ãƒ ã€‚
    - **3: é«˜**: **æ·±ã„çŸ¥è­˜ãŒãªã„ã¨æ„å‘³ä¸æ˜ã€‚**

    æœ€å¾Œã«ç·åˆã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦ã€è©•ä¾¡ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

    # å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONï¼‰
    å¿…ãš **æœ‰åŠ¹ãªJSONå½¢å¼** ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    {{
      "æ”»æ’ƒæ€§": {{"score": 0-3 }},
      "æŒ‘ç™ºæ€§": {{"score": 0-3 }},
      "æœ‰ç”¨æ€§": {{"score": 0-3 }},
      "æ„Ÿæƒ…æ¥µæ€§": {{"score": -2ã€œ+2 }},
      "è‡ªå·±é¡•ç¤ºæ€§": {{"score": 0-3 }},
      "æ–‡è„ˆä¾å­˜æ€§": {{"score": 0-3 }},
      "ç·åˆã‚³ãƒ¡ãƒ³ãƒˆ": "..."
    }}

    # åˆ†æå¯¾è±¡ã‚³ãƒ¡ãƒ³ãƒˆ
    {comment_text}
    """
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"user", "content": prompt}],
            temperature=0.2
        )
        raw = resp.choices[0].message.content.strip()
        
        # ã€æ—§ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ã‚·ãƒ³ãƒ—ãƒ«ãªå‡¦ç†ã«æˆ»ã™ã€‘
        # è¤‡é›‘ãªæ­£è¦è¡¨ç¾ãªã©ã¯ã‚„ã‚ã€ç¢ºå®Ÿã«å‹•ã„ã¦ã„ãŸæ™‚ã®å‡¦ç†ã ã‘ã«ã™ã‚‹
        raw = re.sub(r"```json", "", raw)
        raw = re.sub(r"```", "", raw)
        raw = raw.strip()

        try:
            return json.loads(raw)
        except json.JSONDecodeError:
            return {"raw_output": raw}
    except Exception as e:
        return {"error": str(e)}

# 4. ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---------------------------------------
st.sidebar.header("ğŸ”§ ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆé–¾å€¤ãƒ¬ãƒ³ã‚¸ï¼‰è¨­å®š")

preset = st.sidebar.radio("ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸ã¶", ["ãƒ•ã‚£ãƒ«ã‚¿ãªã—", "å¹³å’Œãƒ¢ãƒ¼ãƒ‰", "è­°è«–ãƒ¢ãƒ¼ãƒ‰", "ã‚«ã‚¹ã‚¿ãƒ "], index=0)

if preset == "ãƒ•ã‚£ãƒ«ã‚¿ãªã—":
    preset_ranges = {
        "æ”»æ’ƒæ€§": (0,3), "æŒ‘ç™ºæ€§": (0,3), "æœ‰ç”¨æ€§": (0,3), "æ„Ÿæƒ…æ¥µæ€§": (-2,2),
        "è‡ªå·±é¡•ç¤ºæ€§": (0,3), "æ–‡è„ˆä¾å­˜æ€§": (0,3)
    }
elif preset == "å¹³å’Œãƒ¢ãƒ¼ãƒ‰":
    preset_ranges = {
        "æ”»æ’ƒæ€§": (0,1), "æŒ‘ç™ºæ€§": (0,1), "æœ‰ç”¨æ€§": (0,3), "æ„Ÿæƒ…æ¥µæ€§": (0,2),
        "è‡ªå·±é¡•ç¤ºæ€§": (0,3), "æ–‡è„ˆä¾å­˜æ€§": (0,3)
    }
elif preset == "è­°è«–ãƒ¢ãƒ¼ãƒ‰":
    preset_ranges = {
        "æ”»æ’ƒæ€§": (0,2), "æŒ‘ç™ºæ€§": (0,2), "æœ‰ç”¨æ€§": (1,3), "æ„Ÿæƒ…æ¥µæ€§": (-2,2),
        "è‡ªå·±é¡•ç¤ºæ€§": (0,3), "æ–‡è„ˆä¾å­˜æ€§": (0,3)
    }
else:
    preset_ranges = {f["key"]:(f["min"], f["max"]) for f in FEATURES}

# ã€ä¿®æ­£ã€‘ã‚«ã‚¹ã‚¿ãƒ ä»¥å¤–ã¯ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’æ“ä½œä¸å¯ã«ã™ã‚‹
is_disabled = (preset != "ã‚«ã‚¹ã‚¿ãƒ ")

threshold_ranges = {}
with st.sidebar.expander("å„ç‰¹å¾´é‡ã®èª¬æ˜ã¨é–¾å€¤è¨­å®šï¼ˆç¯„å›²ï¼‰", expanded=True):
    for f in FEATURES:
        key = f["key"]
        st.markdown(f"**{key}** â€” {f['desc']}")
        min_v, max_v = f["min"], f["max"]
        init_min, init_max = preset_ranges.get(key, (min_v, max_v))
        
        # disabledãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        rng = st.slider(
            f"{key} ã®è¨±å®¹ãƒ¬ãƒ³ã‚¸", 
            min_v, max_v, 
            (init_min, init_max),
            disabled=is_disabled
        )
        threshold_ranges[key] = rng

# 5. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ------------------------------

if "selected_video_id" not in st.session_state:
    st.session_state["selected_video_id"] = None
if "search_results" not in st.session_state:
    st.session_state["search_results"] = []
if "next_page_token" not in st.session_state:
    st.session_state["next_page_token"] = None

# ã€ã‚·ãƒ¼ãƒ³1ã€‘å‹•ç”»æœªé¸æŠæ™‚ï¼ˆæ¤œç´¢ç”»é¢ï¼‰
if st.session_state["selected_video_id"] is None:
    st.markdown("### 1. å‹•ç”»ã‚’æ¤œç´¢")
    
    col1, col2 = st.columns([4, 1])
    with col1:
        query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›", value="", label_visibility="collapsed", placeholder="ä¾‹: AI è§£èª¬")
    with col2:
        search_btn = st.button("æ¤œç´¢", use_container_width=True)
    
    if search_btn and query:
        st.session_state["search_results"] = []
        st.session_state["next_page_token"] = None
        results, token = search_videos(query, max_results=12) 
        st.session_state["search_results"] = results
        st.session_state["next_page_token"] = token

    if st.session_state["search_results"]:
        videos = st.session_state["search_results"]
        st.markdown(f"#### æ¤œç´¢çµæœ ({len(videos)}ä»¶ è¡¨ç¤ºä¸­)")
        
        N_COLS = 3
        for i in range(0, len(videos), N_COLS):
            cols = st.columns(N_COLS)
            for j in range(N_COLS):
                if i + j < len(videos):
                    v = videos[i + j]
                    with cols[j]:
                        if v["thumbnail"]:
                            st.image(v["thumbnail"], use_container_width=True)
                        title_disp = v["title"]
                        if len(title_disp) > 30: title_disp = title_disp[:30] + "..."
                        st.caption(title_disp)
                        
                        # ã€ä¿®æ­£ã€‘é‡è¤‡ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼ˆkeyã«indexã‚’è¿½åŠ ï¼‰
                        if st.button("é¸æŠ", key=f"select_{v['video_id']}_{i+j}"):
                            st.session_state["selected_video_id"] = v["video_id"]
                            st.session_state["selected_title"] = v["title"]
                            st.rerun()

        if st.session_state["next_page_token"]:
            st.divider()
            if st.button("â¬‡ï¸ ã‚‚ã£ã¨å‹•ç”»ã‚’èª­ã¿è¾¼ã‚€"):
                new_results, new_token = search_videos(
                    query, max_results=12, page_token=st.session_state["next_page_token"]
                )
                if new_results:
                    st.session_state["search_results"].extend(new_results)
                    st.session_state["next_page_token"] = new_token
                    st.rerun()

# ã€ã‚·ãƒ¼ãƒ³2ã€‘å‹•ç”»é¸æŠå¾Œï¼ˆåˆ†æç”»é¢ï¼‰
else:
    vid = st.session_state["selected_video_id"]
    st.button("ğŸ”™ æ¤œç´¢ã«æˆ»ã‚‹", on_click=lambda: st.session_state.update({"selected_video_id": None}))
    st.markdown(f"### ğŸï¸ é¸æŠä¸­: {st.session_state.get('selected_title','(no title)')}")
    st.video(f"https://www.youtube.com/watch?v={vid}")

    if st.button("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã‚’å®Ÿè¡Œï¼ˆä¸Šé™100ä»¶ï¼‰"):
        with st.spinner("ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¦GPTã§åˆ†æã—ã¦ã„ã¾ã™...ï¼ˆæ•°åç§’ã€œæ•°åˆ†ï¼‰"):
            # 120ä»¶å–å¾—
            comments = get_comments(vid, max_comments=120)
            if not comments:
                st.error("ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚³ãƒ¡ãƒ³ãƒˆç„¡åŠ¹ã¾ãŸã¯APIåˆ¶é™ã®å¯èƒ½æ€§ï¼‰")
            else:
                rows = []
                progress_bar = st.progress(0)
                
                # ä¸¦åˆ—å‡¦ç†ï¼ˆæ—§ã‚³ãƒ¼ãƒ‰ã¨åŒã˜Worker 10, ã‚¨ãƒ©ãƒ¼ã¯passï¼‰
                with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                    future_to_comment = {executor.submit(analyze_comment, c): c for c in comments}
                    
                    for i, future in enumerate(concurrent.futures.as_completed(future_to_comment)):
                        c = future_to_comment[future]
                        try:
                            analysis = future.result()
                            row = normalize_analysis_to_row(analysis)
                            row["ã‚³ãƒ¡ãƒ³ãƒˆ"] = c
                            rows.append(row)
                        except Exception as e:
                            pass 
                        
                        progress_bar.progress((i + 1) / len(comments))

                df = pd.DataFrame(rows)
                st.session_state["analysis_df_raw"] = df
                
                # ã€ä¿®æ­£ã€‘120ä»¶å–ã‚Œã¦ã‚‚è¡¨ç¤ºã¯ã€Œ100ä»¶ã€ã«è¦‹ã›ã‚‹
                display_msg_len = min(len(df), 100)
                st.success(f"{display_msg_len} ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã¾ã—ãŸã€‚")

# 6. çµæœè¡¨ç¤º ---------------------------------
if "analysis_df_raw" in st.session_state and st.session_state["analysis_df_raw"] is not None:
    df_raw = st.session_state["analysis_df_raw"]
    df = df_raw.copy()

    mask = pd.Series([True] * len(df))
    for f in FEATURES:
        key = f["key"]
        score_col = f"{key}_score"
        low, high = threshold_ranges.get(key, (f["min"], f["max"]))
        if score_col in df.columns:
            s = pd.to_numeric(df[score_col], errors="coerce")
            # ã€ä¿®æ­£ã€‘NaNã‚‚è¡¨ç¤ºå€™è£œã«æ®‹ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã§æ¶ˆãˆãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
            mask &= s.isna() | ((s >= float(low)) & (s <= float(high)))
        else:
            mask &= True
    df_filtered = df[mask]

    # ã€ä¿®æ­£ã€‘è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å…ˆé ­100ä»¶ã«çµã‚‹
    if len(df_filtered) > 100:
        df_filtered = df_filtered.head(100)

    # ä»¶æ•°è¡¨ç¤ºã®åˆ†æ¯ã‚‚100ã«åˆã‚ã›ã‚‹
    display_count = len(df_filtered)
    total_display_count = min(len(df), 100)

    st.markdown(f"**è¡¨ç¤ºä»¶æ•°:** {display_count} / {total_display_count} ä»¶ï¼ˆé–¾å€¤ãƒ¬ãƒ³ã‚¸ã§çµã‚Šè¾¼ã¿ï¼‰")

    display_cols = ["ã‚³ãƒ¡ãƒ³ãƒˆ"] + [f"{f['key']}_score" for f in FEATURES] + ["ç·åˆã‚³ãƒ¡ãƒ³ãƒˆ"]
    display_cols = [c for c in display_cols if c in df_filtered.columns]
    
    # ã€æ”¹å–„ã€‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’1ã‹ã‚‰é–‹å§‹
    if len(df_filtered) > 0:
        df_display = df_filtered[display_cols].reset_index(drop=True)
        df_display.index = df_display.index + 1
        st.dataframe(df_display, use_container_width=True)

        st.download_button(
            "ğŸ’¾ ãƒ•ã‚£ãƒ«ã‚¿çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            df_filtered.to_csv(index=False).encode("utf-8"),
            file_name="filtered_comment_analysis.csv",
            mime="text/csv"
        )
    else:
        st.warning("æ¡ä»¶ã«åˆã†ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
